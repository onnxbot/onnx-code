<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>LCOV - onnx-coverage.info - onnx/defs/rnn/defs.cc</title>
  <link rel="stylesheet" type="text/css" href="../../../gcov.css">
</head>

<body>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="title">LCOV - code coverage report</td></tr>
    <tr><td class="ruler"><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>

    <tr>
      <td width="100%">
        <table cellpadding=1 border=0 width="100%">
          <tr>
            <td width="10%" class="headerItem">Current view:</td>
            <td width="35%" class="headerValue"><a href="../../../index.html">top level</a> - <a href="index.html">onnx/defs/rnn</a> - defs.cc<span style="font-size: 80%;"> (source / <a href="defs.cc.func-sort-c.html">functions</a>)</span></td>
            <td width="5%"></td>
            <td width="15%"></td>
            <td width="10%" class="headerCovTableHead">Hit</td>
            <td width="10%" class="headerCovTableHead">Total</td>
            <td width="15%" class="headerCovTableHead">Coverage</td>
          </tr>
          <tr>
            <td class="headerItem">Test:</td>
            <td class="headerValue">onnx-coverage.info</td>
            <td></td>
            <td class="headerItem">Lines:</td>
            <td class="headerCovTableEntry">85</td>
            <td class="headerCovTableEntry">85</td>
            <td class="headerCovTableEntryHi">100.0 %</td>
          </tr>
          <tr>
            <td class="headerItem">Date:</td>
            <td class="headerValue">2018-05-11 14:21:51</td>
            <td></td>
            <td class="headerItem">Functions:</td>
            <td class="headerCovTableEntry">5</td>
            <td class="headerCovTableEntry">5</td>
            <td class="headerCovTableEntryHi">100.0 %</td>
          </tr>
          <tr><td><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>
        </table>
      </td>
    </tr>

    <tr><td class="ruler"><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>
  </table>

  <table cellpadding=0 cellspacing=0 border=0>
    <tr>
      <td><br></td>
    </tr>
    <tr>
      <td>
<pre class="sourceHeading">          Line data    Source code</pre>
<pre class="source">
<a name="1"><span class="lineNum">       1 </span>            : // Copyright (c) Facebook Inc. and Microsoft Corporation.</a>
<span class="lineNum">       2 </span>            : // Licensed under the MIT license.
<span class="lineNum">       3 </span>            : 
<span class="lineNum">       4 </span>            : #include &quot;onnx/defs/schema.h&quot;
<span class="lineNum">       5 </span>            : 
<span class="lineNum">       6 </span>            : using namespace ONNX_NAMESPACE;
<span class="lineNum">       7 </span>            : 
<span class="lineNum">       8 </span>            : namespace ONNX_NAMESPACE {
<a name="9"><span class="lineNum">       9 </span>            : </a>
<a name="10"><span class="lineNum">      10 </span>            : // Warning: This function may be shared with old versions in old.cc.</a>
<span class="lineNum">      11 </span>            : std::function&lt;void(OpSchema&amp;)&gt; RNNDocGenerator(const char* /*name*/) {
<span class="lineNum">      12 </span><span class="lineCov">          3 :     return [=](OpSchema&amp; schema) {</span>
<span class="lineNum">      13 </span><span class="lineCov">          9 :         schema.Attr(&quot;direction&quot;, &quot;Specify if the RNN is forward, reverse, or bidirectional. &quot;</span>
<span class="lineNum">      14 </span>            :                     &quot;Must be one of forward (default), reverse, or bidirectional.&quot;,
<span class="lineNum">      15 </span>            :                     AttributeProto::STRING,
<span class="lineNum">      16 </span><span class="lineCov">          3 :                     std::string(&quot;forward&quot;));</span>
<span class="lineNum">      17 </span><span class="lineCov">          9 :         schema.Attr(&quot;hidden_size&quot;, &quot;Number of neurons in the hidden layer&quot;, AttributeProto::INT, OPTIONAL);</span>
<span class="lineNum">      18 </span><span class="lineCov">          9 :         schema.Attr(&quot;activation_alpha&quot;,</span>
<span class="lineNum">      19 </span><span class="lineCov">          3 :                     &quot;Optional scaling values used by some activation functions. The values &quot;</span>
<span class="lineNum">      20 </span>            :                     &quot;are consumed in the order of activation functions, for example (f, g, h) &quot;
<span class="lineNum">      21 </span>            :                     &quot;in LSTM. Default values are the same as of corresponding ONNX operators.&quot;
<span class="lineNum">      22 </span>            :                     &quot;For example with LeakyRelu, the default alpha is 0.01.&quot;,
<span class="lineNum">      23 </span>            :                     AttributeProto::FLOATS,
<span class="lineNum">      24 </span>            :                     OPTIONAL);
<span class="lineNum">      25 </span><span class="lineCov">          9 :         schema.Attr(&quot;activation_beta&quot;,</span>
<span class="lineNum">      26 </span><span class="lineCov">          3 :                     &quot;Optional scaling values used by some activation functions. The values &quot;</span>
<span class="lineNum">      27 </span>            :                     &quot;are consumed in the order of activation functions, for example (f, g, h) &quot;
<span class="lineNum">      28 </span>            :                     &quot;in LSTM. Default values are the same as of corresponding ONNX operators.&quot;,
<span class="lineNum">      29 </span>            :                     AttributeProto::FLOATS,
<span class="lineNum">      30 </span>            :                     OPTIONAL);
<span class="lineNum">      31 </span><span class="lineCov">          9 :         schema.Attr(&quot;output_sequence&quot;,</span>
<span class="lineNum">      32 </span><span class="lineCov">          3 :                     &quot;The sequence output for the hidden is optional if 0. Default 0.&quot;,</span>
<span class="lineNum">      33 </span>            :                     AttributeProto::INT,
<span class="lineNum">      34 </span><span class="lineCov">          3 :                     static_cast&lt;int64_t&gt;(0));</span>
<span class="lineNum">      35 </span><span class="lineCov">          9 :         schema.Attr(&quot;clip&quot;, &quot;Cell clip threshold. Clipping bounds the elements of a tensor &quot;</span>
<span class="lineNum">      36 </span>            :                     &quot;in the range of [-threshold, +threshold] and is applied to the input &quot;
<span class="lineNum">      37 </span>            :                     &quot;of activations. No clip if not specified.&quot;, AttributeProto::FLOAT, OPTIONAL);
<span class="lineNum">      38 </span><span class="lineCov">          9 :         schema.Input(0, &quot;X&quot;,</span>
<span class="lineNum">      39 </span><span class="lineCov">          3 :                      &quot;The input sequences packed (and potentially padded) into one 3-D &quot;</span>
<span class="lineNum">      40 </span><span class="lineCov">          3 :                      &quot;tensor with the shape of `[seq_length, batch_size, input_size]`.&quot;, &quot;T&quot;);</span>
<span class="lineNum">      41 </span><span class="lineCov">          9 :         schema.Input(4, &quot;sequence_lens&quot;,</span>
<span class="lineNum">      42 </span><span class="lineCov">          3 :                      &quot;Optional tensor specifying lengths of the sequences in a batch. &quot;</span>
<span class="lineNum">      43 </span>            :                      &quot;If not specified - assumed all sequences in the batch to have &quot;
<span class="lineNum">      44 </span><span class="lineCov">          3 :                      &quot;length `seq_length`. It has shape `[batch_size]`.&quot;, &quot;T1&quot;,</span>
<span class="lineNum">      45 </span>            :                      OpSchema::Optional);
<span class="lineNum">      46 </span><span class="lineCov">          9 :         schema.Input(5, &quot;initial_h&quot;,</span>
<span class="lineNum">      47 </span><span class="lineCov">          3 :                      &quot;Optional initial value of the hidden. If not specified - assumed &quot;</span>
<span class="lineNum">      48 </span>            :                      &quot;to be 0. It has shape `[num_directions, batch_size, hidden_size]`.&quot;,
<span class="lineNum">      49 </span><span class="lineCov">          3 :                      &quot;T&quot;, OpSchema::Optional);</span>
<span class="lineNum">      50 </span><span class="lineCov">          9 :         schema.Output(0, &quot;Y&quot;,</span>
<span class="lineNum">      51 </span><span class="lineCov">          3 :                       &quot;A tensor that concats all the intermediate output values of the hidden. &quot;</span>
<span class="lineNum">      52 </span>            :                       &quot;It has shape `[seq_length, num_directions, batch_size, hidden_size]`. &quot;
<span class="lineNum">      53 </span><span class="lineCov">          3 :                       &quot;It is optional if `output_sequence` is 0.&quot;, &quot;T&quot;, OpSchema::Optional);</span>
<span class="lineNum">      54 </span><span class="lineCov">          9 :         schema.Output(1, &quot;Y_h&quot;,</span>
<span class="lineNum">      55 </span><span class="lineCov">          3 :                       &quot;The last output value of the hidden. It has shape &quot;</span>
<span class="lineNum">      56 </span><span class="lineCov">          3 :                       &quot;`[num_directions, batch_size, hidden_size]`.&quot;, &quot;T&quot;, OpSchema::Optional);</span>
<span class="lineNum">      57 </span><span class="lineCov">         30 :         schema.TypeConstraint(&quot;T&quot;, { &quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot; },</span>
<span class="lineNum">      58 </span><span class="lineCov">          3 :                               &quot;Constrain input and output types to float tensors.&quot;);</span>
<span class="lineNum">      59 </span><span class="lineCov">         21 :         schema.TypeConstraint(&quot;T1&quot;, { &quot;tensor(int32)&quot; }, &quot;Constrain seq_lens to integer tensor.&quot;);</span>
<span class="lineNum">      60 </span><span class="lineCov">          3 :     };</span>
<a name="61"><span class="lineNum">      61 </span>            : }</a>
<span class="lineNum">      62 </span>            : 
<span class="lineNum">      63 </span><span class="lineCov">          8 : ONNX_OPERATOR_SCHEMA(RNN)</span>
<span class="lineNum">      64 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">      65 </span>            : Computes an one-layer simple RNN. This operator is usually supported
<span class="lineNum">      66 </span>            : via some custom implementation such as CuDNN.
<span class="lineNum">      67 </span>            : 
<span class="lineNum">      68 </span>            : Notations:
<span class="lineNum">      69 </span>            : 
<span class="lineNum">      70 </span>            : `X` - input tensor
<span class="lineNum">      71 </span>            : 
<span class="lineNum">      72 </span>            : `i` - input gate
<span class="lineNum">      73 </span>            : 
<span class="lineNum">      74 </span>            : `t` - time step (t-1 means previous time step)
<span class="lineNum">      75 </span>            : 
<span class="lineNum">      76 </span>            : `Wi` - W parameter weight matrix for input gate
<span class="lineNum">      77 </span>            : 
<span class="lineNum">      78 </span>            : `Ri` - R recurrence weight matrix for input gate
<span class="lineNum">      79 </span>            : 
<span class="lineNum">      80 </span>            : `Wbi` - W parameter bias vector for input gate
<span class="lineNum">      81 </span>            : 
<span class="lineNum">      82 </span>            : `Rbi` - R parameter bias vector for input gate
<span class="lineNum">      83 </span>            : 
<span class="lineNum">      84 </span>            : `WBi` - W parameter weight matrix for backward input gate
<span class="lineNum">      85 </span>            : 
<span class="lineNum">      86 </span>            : `RBi` - R recurrence weight matrix for backward input gate
<span class="lineNum">      87 </span>            : 
<span class="lineNum">      88 </span>            : `WBbi` - WR bias vectors for backward input gate
<span class="lineNum">      89 </span>            : 
<span class="lineNum">      90 </span>            : `RBbi` - RR bias vectors for backward input gate
<span class="lineNum">      91 </span>            : 
<span class="lineNum">      92 </span>            : `H` - Hidden state
<span class="lineNum">      93 </span>            : 
<span class="lineNum">      94 </span>            : `num_directions` - 2 if direction == bidirectional else 1
<span class="lineNum">      95 </span>            : 
<span class="lineNum">      96 </span>            : Activation functions:
<span class="lineNum">      97 </span>            : 
<span class="lineNum">      98 </span>            :   Relu(x)                - max(0, x)
<span class="lineNum">      99 </span>            : 
<span class="lineNum">     100 </span>            :   Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})
<span class="lineNum">     101 </span>            : 
<span class="lineNum">     102 </span>            :   Sigmoid(x)             - 1/(1 + e^{-x})
<span class="lineNum">     103 </span>            : 
<span class="lineNum">     104 </span>            :   (NOTE: Below are optional)
<span class="lineNum">     105 </span>            : 
<span class="lineNum">     106 </span>            :   Affine(x)              - alpha*x + beta
<span class="lineNum">     107 </span>            : 
<span class="lineNum">     108 </span>            :   LeakyRelu(x)           - x if x &gt;= 0 else alpha * x
<span class="lineNum">     109 </span>            : 
<span class="lineNum">     110 </span>            :   ThresholdedRelu(x)     - x if x &gt;= alpha else 0
<span class="lineNum">     111 </span>            : 
<span class="lineNum">     112 </span>            :   ScaledTanh(x)          - alpha*Tanh(beta*x)
<span class="lineNum">     113 </span>            : 
<span class="lineNum">     114 </span>            :   HardSigmoid(x)         - min(max(alpha*x + beta, 0), 1)
<span class="lineNum">     115 </span>            : 
<span class="lineNum">     116 </span>            :   Elu(x)                 - x if x &gt;= 0 else alpha*(e^x - 1)
<span class="lineNum">     117 </span>            : 
<span class="lineNum">     118 </span>            :   Softsign(x)            - x/(1 + |x|)
<span class="lineNum">     119 </span>            : 
<span class="lineNum">     120 </span>            :   Softplus(x)            - log(1 + e^x)
<span class="lineNum">     121 </span>            : 
<span class="lineNum">     122 </span>            : Equations (Default: f=Tanh):
<span class="lineNum">     123 </span>            : 
<span class="lineNum">     124 </span>            :   - Ht = f(Xt*(Wi^T) + Ht-1*Ri + Wbi + Rbi)
<span class="lineNum">     125 </span>            : )DOC&quot;)
<span class="lineNum">     126 </span><span class="lineCov">          3 :     .Attr(&quot;activations&quot;, &quot;One (or two if bidirectional) activation function for &quot;</span>
<span class="lineNum">     127 </span>            :           &quot;input gate. The activation function must be one of the activation &quot;
<span class="lineNum">     128 </span>            :           &quot;functions specified above. Optional: Default `Tanh` if not specified.&quot;,
<span class="lineNum">     129 </span>            :           AttributeProto::STRINGS,
<span class="lineNum">     130 </span><span class="lineCov">          3 :           std::vector&lt;std::string&gt;{&quot;Tanh&quot;, &quot;Tanh&quot;})</span>
<span class="lineNum">     131 </span><span class="lineCov">          2 :     .Input(1, &quot;W&quot;,</span>
<span class="lineNum">     132 </span><span class="lineCov">          1 :            &quot;The weight tensor for input gate. Concatenation of `Wi` and `WBi` &quot;</span>
<span class="lineNum">     133 </span>            :            &quot;(if bidirectional). The tensor has shape &quot;
<span class="lineNum">     134 </span><span class="lineCov">          1 :            &quot;`[num_directions, hidden_size, input_size]`.&quot;, &quot;T&quot;)</span>
<span class="lineNum">     135 </span><span class="lineCov">          2 :     .Input(2, &quot;R&quot;,</span>
<span class="lineNum">     136 </span><span class="lineCov">          1 :            &quot;The recurrence weight tensor. Concatenation of `Ri` and `RBi` &quot;</span>
<span class="lineNum">     137 </span>            :            &quot;(if bidirectional). The tensor has shape &quot;
<span class="lineNum">     138 </span><span class="lineCov">          1 :            &quot;`[num_directions, hidden_size, hidden_size]`.&quot;, &quot;T&quot;)</span>
<span class="lineNum">     139 </span><span class="lineCov">          2 :     .Input(3, &quot;B&quot;,</span>
<span class="lineNum">     140 </span><span class="lineCov">          1 :            &quot;The bias tensor for input gate. Concatenation of `[Wbi, Rbi]` &quot;</span>
<span class="lineNum">     141 </span>            :            &quot;and `[WBbi, RBbi]` (if bidirectional). The tensor has shape &quot;
<span class="lineNum">     142 </span>            :            &quot;`[num_directions, 2*hidden_size]`. Optional: If not specified - assumed &quot;
<span class="lineNum">     143 </span><span class="lineCov">          1 :            &quot;to be 0.&quot;, &quot;T&quot;,</span>
<span class="lineNum">     144 </span>            :         OpSchema::Optional)
<span class="lineNum">     145 </span><span class="lineCov">          2 :     .FillUsing(RNNDocGenerator(&quot;RNN&quot;));</span>
<a name="146"><span class="lineNum">     146 </span>            : </a>
<span class="lineNum">     147 </span>            : 
<span class="lineNum">     148 </span><span class="lineCov">          5 : ONNX_OPERATOR_SCHEMA(GRU)</span>
<span class="lineNum">     149 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     150 </span>            : Computes an one-layer GRU. This operator is usually supported via some custom
<span class="lineNum">     151 </span>            : implementation such as CuDNN.
<span class="lineNum">     152 </span>            : 
<span class="lineNum">     153 </span>            : Notations:
<span class="lineNum">     154 </span>            : 
<span class="lineNum">     155 </span>            : `X` - input tensor
<span class="lineNum">     156 </span>            : 
<span class="lineNum">     157 </span>            : `z` - update gate
<span class="lineNum">     158 </span>            : 
<span class="lineNum">     159 </span>            : `r` - reset gate
<span class="lineNum">     160 </span>            : 
<span class="lineNum">     161 </span>            : `h` - hidden gate
<span class="lineNum">     162 </span>            : 
<span class="lineNum">     163 </span>            : `t` - time step (t-1 means previous time step)
<span class="lineNum">     164 </span>            : 
<span class="lineNum">     165 </span>            : `W[zrh]` - W parameter weight matrix for update, reset, and hidden gates
<span class="lineNum">     166 </span>            : 
<span class="lineNum">     167 </span>            : `R[zrh]` - R recurrence weight matrix for update, reset, and hidden gates
<span class="lineNum">     168 </span>            : 
<span class="lineNum">     169 </span>            : `Wb[zrh]` - W bias vectors for update, reset, and hidden gates
<span class="lineNum">     170 </span>            : 
<span class="lineNum">     171 </span>            : `Rb[zrh]` - R bias vectors for update, reset, and hidden gates
<span class="lineNum">     172 </span>            : 
<span class="lineNum">     173 </span>            : `WB[zrh]` - W parameter weight matrix for backward update, reset, and hidden gates
<span class="lineNum">     174 </span>            : 
<span class="lineNum">     175 </span>            : `RB[zrh]` - R recurrence weight matrix for backward update, reset, and hidden gates
<span class="lineNum">     176 </span>            : 
<span class="lineNum">     177 </span>            : `WBb[zrh]` - W bias vectors for backward update, reset, and hidden gates
<span class="lineNum">     178 </span>            : 
<span class="lineNum">     179 </span>            : `RBb[zrh]` - R bias vectors for backward update, reset, and hidden gates
<span class="lineNum">     180 </span>            : 
<span class="lineNum">     181 </span>            : `H` - Hidden state
<span class="lineNum">     182 </span>            : 
<span class="lineNum">     183 </span>            : `num_directions` - 2 if direction == bidirectional else 1
<span class="lineNum">     184 </span>            : 
<span class="lineNum">     185 </span>            : Activation functions:
<span class="lineNum">     186 </span>            : 
<span class="lineNum">     187 </span>            :   Relu(x)                - max(0, x)
<span class="lineNum">     188 </span>            : 
<span class="lineNum">     189 </span>            :   Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})
<span class="lineNum">     190 </span>            : 
<span class="lineNum">     191 </span>            :   Sigmoid(x)             - 1/(1 + e^{-x})
<span class="lineNum">     192 </span>            : 
<span class="lineNum">     193 </span>            :   (NOTE: Below are optional)
<span class="lineNum">     194 </span>            : 
<span class="lineNum">     195 </span>            :   Affine(x)              - alpha*x + beta
<span class="lineNum">     196 </span>            : 
<span class="lineNum">     197 </span>            :   LeakyRelu(x)           - x if x &gt;= 0 else alpha * x
<span class="lineNum">     198 </span>            : 
<span class="lineNum">     199 </span>            :   ThresholdedRelu(x)     - x if x &gt;= alpha else 0
<span class="lineNum">     200 </span>            : 
<span class="lineNum">     201 </span>            :   ScaledTanh(x)          - alpha*Tanh(beta*x)
<span class="lineNum">     202 </span>            : 
<span class="lineNum">     203 </span>            :   HardSigmoid(x)         - min(max(alpha*x + beta, 0), 1)
<span class="lineNum">     204 </span>            : 
<span class="lineNum">     205 </span>            :   Elu(x)                 - x if x &gt;= 0 else alpha*(e^x - 1)
<span class="lineNum">     206 </span>            : 
<span class="lineNum">     207 </span>            :   Softsign(x)            - x/(1 + |x|)
<span class="lineNum">     208 </span>            : 
<span class="lineNum">     209 </span>            :   Softplus(x)            - log(1 + e^x)
<span class="lineNum">     210 </span>            : 
<span class="lineNum">     211 </span>            : Equations (Default: f=Sigmoid, g=Tanh):
<span class="lineNum">     212 </span>            : 
<span class="lineNum">     213 </span>            :   - zt = f(Xt*(Wz^T) + Ht-1*Rz + Wbz + Rbz)
<span class="lineNum">     214 </span>            : 
<span class="lineNum">     215 </span>            :   - rt = f(Xt*(Wr^T) + Ht-1*Rr + Wbr + Rbr)
<span class="lineNum">     216 </span>            : 
<span class="lineNum">     217 </span>            :   - ht = g(Xt*(Wh^T) + (rt (.) Ht-1)*Rh + Rbh + Wbh) # default, when linear_before_reset = 0
<span class="lineNum">     218 </span>            : 
<span class="lineNum">     219 </span>            :   - ht = g(Xt*(Wh^T) + (rt (.) (Ht-1*Rh + Rbh) + Wbh) # when linear_before_reset != 0
<span class="lineNum">     220 </span>            : 
<span class="lineNum">     221 </span>            :   - Ht = (1 - zt) (.) ht + zt (.) Ht-1
<span class="lineNum">     222 </span>            : )DOC&quot;)
<span class="lineNum">     223 </span><span class="lineCov">          3 :     .Attr(&quot;activations&quot;, &quot;A list of 2 (or 4 if bidirectional) activation functions &quot;</span>
<span class="lineNum">     224 </span>            :           &quot;for update, reset, and hidden gates. The activation functions must be one &quot;
<span class="lineNum">     225 </span>            :           &quot;of the activation functions specified above. Optional: See the equations &quot;
<span class="lineNum">     226 </span>            :           &quot;for default if not specified.&quot;,
<span class="lineNum">     227 </span>            :           AttributeProto::STRINGS,
<span class="lineNum">     228 </span>            :           OPTIONAL)
<span class="lineNum">     229 </span><span class="lineCov">          1 :     .SinceVersion(3)</span>
<span class="lineNum">     230 </span><span class="lineCov">          3 :     .Attr(&quot;linear_before_reset&quot;, &quot;When computing the output of the hidden gate, &quot;</span>
<span class="lineNum">     231 </span>            :           &quot;apply the linear transformation before multiplying by the output of the &quot;
<span class="lineNum">     232 </span>            :           &quot;reset gate.&quot;,
<span class="lineNum">     233 </span>            :           AttributeProto::INT,
<span class="lineNum">     234 </span><span class="lineCov">          1 :           static_cast&lt;int64_t&gt;(0))</span>
<span class="lineNum">     235 </span><span class="lineCov">          2 :     .Input(1, &quot;W&quot;,</span>
<span class="lineNum">     236 </span><span class="lineCov">          1 :            &quot;The weight tensor for the gates. Concatenation of `W[zrh]` and `WB[zrh]` &quot;</span>
<span class="lineNum">     237 </span>            :            &quot;(if bidirectional) along dimension 0. This tensor has shape &quot;
<span class="lineNum">     238 </span><span class="lineCov">          1 :            &quot;`[num_directions, 3*hidden_size, input_size]`.&quot;, &quot;T&quot;)</span>
<span class="lineNum">     239 </span><span class="lineCov">          2 :     .Input(2, &quot;R&quot;,</span>
<span class="lineNum">     240 </span><span class="lineCov">          1 :            &quot;The recurrence weight tensor. Concatenation of `R[zrh]` and `RB[zrh]` &quot;</span>
<span class="lineNum">     241 </span>            :            &quot;(if bidirectional) along dimension 0. This tensor has shape &quot;
<span class="lineNum">     242 </span><span class="lineCov">          1 :            &quot;`[num_directions, 3*hidden_size, hidden_size]`.&quot;, &quot;T&quot;)</span>
<span class="lineNum">     243 </span><span class="lineCov">          2 :     .Input(3, &quot;B&quot;,</span>
<span class="lineNum">     244 </span><span class="lineCov">          1 :            &quot;The bias tensor for the gates. Concatenation of `[Wb[zrh], Rb[zrh]]` and &quot;</span>
<span class="lineNum">     245 </span>            :            &quot;`[WBb[zrh], RBb[zrh]]` (if bidirectional) along dimension 0. This tensor &quot;
<span class="lineNum">     246 </span>            :            &quot;has shape `[num_directions, 6*hidden_size]`. Optional: If not specified &quot;
<span class="lineNum">     247 </span><span class="lineCov">          1 :            &quot;- assumed to be 0&quot;, &quot;T&quot;,</span>
<span class="lineNum">     248 </span>            :         OpSchema::Optional)
<span class="lineNum">     249 </span><span class="lineCov">          2 :     .FillUsing(RNNDocGenerator(&quot;GRU&quot;));</span>
<a name="250"><span class="lineNum">     250 </span>            : </a>
<span class="lineNum">     251 </span>            : 
<span class="lineNum">     252 </span><span class="lineCov">          5 : ONNX_OPERATOR_SCHEMA(LSTM)</span>
<span class="lineNum">     253 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     254 </span>            : Computes an one-layer LSTM. This operator is usually supported via some
<span class="lineNum">     255 </span>            : custom implementation such as CuDNN.
<span class="lineNum">     256 </span>            : 
<span class="lineNum">     257 </span>            : Notations:
<span class="lineNum">     258 </span>            : 
<span class="lineNum">     259 </span>            : `X` - input tensor
<span class="lineNum">     260 </span>            : 
<span class="lineNum">     261 </span>            : `i` - input gate
<span class="lineNum">     262 </span>            : 
<span class="lineNum">     263 </span>            : `o` - output gate
<span class="lineNum">     264 </span>            : 
<span class="lineNum">     265 </span>            : `f` - forget gate
<span class="lineNum">     266 </span>            : 
<span class="lineNum">     267 </span>            : `c` - cell gate
<span class="lineNum">     268 </span>            : 
<span class="lineNum">     269 </span>            : `t` - time step (t-1 means previous time step)
<span class="lineNum">     270 </span>            : 
<span class="lineNum">     271 </span>            : `W[iofc]` - W parameter weight matrix for input, output, forget, and cell gates
<span class="lineNum">     272 </span>            : 
<span class="lineNum">     273 </span>            : `R[iofc]` - R recurrence weight matrix for input, output, forget, and cell gates
<span class="lineNum">     274 </span>            : 
<span class="lineNum">     275 </span>            : `Wb[iofc]` - W bias vectors for input, output, forget, and cell gates
<span class="lineNum">     276 </span>            : 
<span class="lineNum">     277 </span>            : `Rb[iofc]` - R bias vectors for input, output, forget, and cell gates
<span class="lineNum">     278 </span>            : 
<span class="lineNum">     279 </span>            : `P[iof]`  - P peephole weight vector for input, output, and forget gates
<span class="lineNum">     280 </span>            : 
<span class="lineNum">     281 </span>            : `WB[iofc]` - W parameter weight matrix for backward input, output, forget, and cell gates
<span class="lineNum">     282 </span>            : 
<span class="lineNum">     283 </span>            : `RB[iofc]` - R recurrence weight matrix for backward input, output, forget, and cell gates
<span class="lineNum">     284 </span>            : 
<span class="lineNum">     285 </span>            : `WBb[iofc]` - W bias vectors for backward input, output, forget, and cell gates
<span class="lineNum">     286 </span>            : 
<span class="lineNum">     287 </span>            : `RBb[iofc]` - R bias vectors for backward input, output, forget, and cell gates
<span class="lineNum">     288 </span>            : 
<span class="lineNum">     289 </span>            : `PB[iof]`  - P peephole weight vector for backward input, output, and forget gates
<span class="lineNum">     290 </span>            : 
<span class="lineNum">     291 </span>            : `H` - Hidden state
<span class="lineNum">     292 </span>            : 
<span class="lineNum">     293 </span>            : `num_directions` - 2 if direction == bidirectional else 1
<span class="lineNum">     294 </span>            : 
<span class="lineNum">     295 </span>            : Activation functions:
<span class="lineNum">     296 </span>            : 
<span class="lineNum">     297 </span>            :   Relu(x)                - max(0, x)
<span class="lineNum">     298 </span>            : 
<span class="lineNum">     299 </span>            :   Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})
<span class="lineNum">     300 </span>            : 
<span class="lineNum">     301 </span>            :   Sigmoid(x)             - 1/(1 + e^{-x})
<span class="lineNum">     302 </span>            : 
<span class="lineNum">     303 </span>            :   (NOTE: Below are optional)
<span class="lineNum">     304 </span>            : 
<span class="lineNum">     305 </span>            :   Affine(x)              - alpha*x + beta
<span class="lineNum">     306 </span>            : 
<span class="lineNum">     307 </span>            :   LeakyRelu(x)           - x if x &gt;= 0 else alpha * x
<span class="lineNum">     308 </span>            : 
<span class="lineNum">     309 </span>            :   ThresholdedRelu(x)     - x if x &gt;= alpha else 0
<span class="lineNum">     310 </span>            : 
<span class="lineNum">     311 </span>            :   ScaledTanh(x)          - alpha*Tanh(beta*x)
<span class="lineNum">     312 </span>            : 
<span class="lineNum">     313 </span>            :   HardSigmoid(x)         - min(max(alpha*x + beta, 0), 1)
<span class="lineNum">     314 </span>            : 
<span class="lineNum">     315 </span>            :   Elu(x)                 - x if x &gt;= 0 else alpha*(e^x - 1)
<span class="lineNum">     316 </span>            : 
<span class="lineNum">     317 </span>            :   Softsign(x)            - x/(1 + |x|)
<span class="lineNum">     318 </span>            : 
<span class="lineNum">     319 </span>            :   Softplus(x)            - log(1 + e^x)
<span class="lineNum">     320 </span>            : 
<span class="lineNum">     321 </span>            : Equations (Default: f=Sigmoid, g=Tanh, h=Tanh):
<span class="lineNum">     322 </span>            : 
<span class="lineNum">     323 </span>            :   - it = f(Xt*(Wi^T) + Ht-1*Ri + Pi (.) Ct-1 + Wbi + Rbi)
<span class="lineNum">     324 </span>            : 
<span class="lineNum">     325 </span>            :   - ft = f(Xt*(Wf^T) + Ht-1*Rf + Pf (.) Ct-1 + Wbf + Rbf)
<span class="lineNum">     326 </span>            : 
<span class="lineNum">     327 </span>            :   - ct = g(Xt*(Wc^T) + Ht-1*Rc + Wbc + Rbc)
<span class="lineNum">     328 </span>            : 
<span class="lineNum">     329 </span>            :   - Ct = ft (.) Ct-1 + it (.) ct
<span class="lineNum">     330 </span>            : 
<span class="lineNum">     331 </span>            :   - ot = f(Xt*(Wo^T) + Ht-1*Ro + Po (.) Ct + Wbo + Rbo)
<span class="lineNum">     332 </span>            : 
<span class="lineNum">     333 </span>            :   - Ht = ot (.) h(Ct)
<span class="lineNum">     334 </span>            : )DOC&quot;)
<span class="lineNum">     335 </span><span class="lineCov">          3 :     .Attr(&quot;activations&quot;, &quot;A list of 3 (or 6 if bidirectional) activation functions &quot;</span>
<span class="lineNum">     336 </span>            :           &quot;for input, output, forget, cell, and hidden. The activation functions must &quot;
<span class="lineNum">     337 </span>            :           &quot;be one of the activation functions specified above. Optional: See the equations &quot;
<span class="lineNum">     338 </span>            :           &quot;for default if not specified.&quot;,
<span class="lineNum">     339 </span>            :           AttributeProto::STRINGS,
<span class="lineNum">     340 </span>            :           OPTIONAL)
<span class="lineNum">     341 </span><span class="lineCov">          3 :     .Attr(&quot;input_forget&quot;, &quot;Couple the input and forget gates if 1, default 0.&quot;,</span>
<span class="lineNum">     342 </span>            :           AttributeProto::INT,
<span class="lineNum">     343 </span><span class="lineCov">          1 :           static_cast&lt;int64_t&gt;(0))</span>
<span class="lineNum">     344 </span><span class="lineCov">          2 :     .Input(1, &quot;W&quot;,</span>
<span class="lineNum">     345 </span><span class="lineCov">          1 :            &quot;The weight tensor for the gates. Concatenation of `W[iofc]` and &quot;</span>
<span class="lineNum">     346 </span>            :            &quot;`WB[iofc]` (if bidirectional) along dimension 0. The tensor has shape &quot;
<span class="lineNum">     347 </span><span class="lineCov">          1 :            &quot;`[num_directions, 4*hidden_size, input_size]`.&quot;, &quot;T&quot;)</span>
<span class="lineNum">     348 </span><span class="lineCov">          2 :     .Input(2, &quot;R&quot;,</span>
<span class="lineNum">     349 </span><span class="lineCov">          1 :            &quot;The recurrence weight tensor. Concatenation of `R[iofc]` and &quot;</span>
<span class="lineNum">     350 </span>            :            &quot;`RB[iofc]` (if bidirectional) along dimension 0. This tensor has shape &quot;
<span class="lineNum">     351 </span><span class="lineCov">          1 :            &quot;`[num_directions, 4*hidden_size, hidden_size]`.&quot;, &quot;T&quot;)</span>
<span class="lineNum">     352 </span><span class="lineCov">          2 :     .Input(3, &quot;B&quot;,</span>
<span class="lineNum">     353 </span><span class="lineCov">          1 :            &quot;The bias tensor for input gate. Concatenation of `[Wb[iofc], Rb[iofc]]`, &quot;</span>
<span class="lineNum">     354 </span>            :            &quot;and `[WBb[iofc], RBb[iofc]]` (if bidirectional) along dimension 0. This &quot;
<span class="lineNum">     355 </span>            :            &quot;tensor has shape `[num_directions, 8*hidden_size]`. Optional: If not &quot;
<span class="lineNum">     356 </span><span class="lineCov">          1 :            &quot;specified - assumed to be 0.&quot;, &quot;T&quot;,</span>
<span class="lineNum">     357 </span>            :        OpSchema::Optional)
<span class="lineNum">     358 </span><span class="lineCov">          2 :     .Input(6, &quot;initial_c&quot;,</span>
<span class="lineNum">     359 </span><span class="lineCov">          1 :            &quot;Optional initial value of the cell. If not specified - assumed &quot;</span>
<span class="lineNum">     360 </span>            :            &quot;to be 0. It has shape `[num_directions, batch_size, hidden_size]`.&quot;,
<span class="lineNum">     361 </span><span class="lineCov">          1 :            &quot;T&quot;, OpSchema::Optional)</span>
<span class="lineNum">     362 </span><span class="lineCov">          2 :     .Input(7, &quot;P&quot;,</span>
<span class="lineNum">     363 </span><span class="lineCov">          1 :            &quot;The weight tensor for peepholes. Concatenation of `P[iof]` and &quot;</span>
<span class="lineNum">     364 </span>            :            &quot;`PB[iof]` (if bidirectional) along dimension 0. It has shape &quot;
<span class="lineNum">     365 </span>            :            &quot;`[num_directions, 3*hidde_size]`. Optional: If not specified - &quot;
<span class="lineNum">     366 </span><span class="lineCov">          1 :            &quot;assumed to be 0.&quot;, &quot;T&quot;,</span>
<span class="lineNum">     367 </span>            :        OpSchema::Optional)
<span class="lineNum">     368 </span><span class="lineCov">          2 :     .FillUsing(RNNDocGenerator(&quot;LSTM&quot;))</span>
<span class="lineNum">     369 </span><span class="lineCov">          2 :     .Output(2, &quot;Y_c&quot;,</span>
<span class="lineNum">     370 </span><span class="lineCov">          1 :             &quot;The last output value of the cell. It has shape &quot;</span>
<span class="lineNum">     371 </span><span class="lineCov">          1 :             &quot;`[num_directions, batch_size, hidden_size]`.&quot;, &quot;T&quot;, OpSchema::Optional);</span>
<span class="lineNum">     372 </span>            : }  // namespace ONNX_NAMESPACE
</pre>
      </td>
    </tr>
  </table>
  <br>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="ruler"><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>
    <tr><td class="versionInfo">Generated by: <a href="http://ltp.sourceforge.net/coverage/lcov.php" target="_parent">LCOV version 1.13</a></td></tr>
  </table>
  <br>

</body>
</html>
