<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>LCOV - onnx-coverage.info - onnx/defs/math/old.cc</title>
  <link rel="stylesheet" type="text/css" href="../../../gcov.css">
</head>

<body>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="title">LCOV - code coverage report</td></tr>
    <tr><td class="ruler"><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>

    <tr>
      <td width="100%">
        <table cellpadding=1 border=0 width="100%">
          <tr>
            <td width="10%" class="headerItem">Current view:</td>
            <td width="35%" class="headerValue"><a href="../../../index.html">top level</a> - <a href="index.html">onnx/defs/math</a> - old.cc<span style="font-size: 80%;"> (source / <a href="old.cc.func-sort-c.html">functions</a>)</span></td>
            <td width="5%"></td>
            <td width="15%"></td>
            <td width="10%" class="headerCovTableHead">Hit</td>
            <td width="10%" class="headerCovTableHead">Total</td>
            <td width="15%" class="headerCovTableHead">Coverage</td>
          </tr>
          <tr>
            <td class="headerItem">Test:</td>
            <td class="headerValue">onnx-coverage.info</td>
            <td></td>
            <td class="headerItem">Lines:</td>
            <td class="headerCovTableEntry">331</td>
            <td class="headerCovTableEntry">331</td>
            <td class="headerCovTableEntryHi">100.0 %</td>
          </tr>
          <tr>
            <td class="headerItem">Date:</td>
            <td class="headerValue">2018-05-11 14:21:51</td>
            <td></td>
            <td class="headerItem">Functions:</td>
            <td class="headerCovTableEntry">28</td>
            <td class="headerCovTableEntry">28</td>
            <td class="headerCovTableEntryHi">100.0 %</td>
          </tr>
          <tr><td><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>
        </table>
      </td>
    </tr>

    <tr><td class="ruler"><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>
  </table>

  <table cellpadding=0 cellspacing=0 border=0>
    <tr>
      <td><br></td>
    </tr>
    <tr>
      <td>
<pre class="sourceHeading">          Line data    Source code</pre>
<pre class="source">
<a name="1"><span class="lineNum">       1 </span>            : // Copyright (c) Facebook Inc. and Microsoft Corporation.</a>
<span class="lineNum">       2 </span>            : // Licensed under the MIT license.
<span class="lineNum">       3 </span>            : 
<span class="lineNum">       4 </span>            : #include &lt;functional&gt;
<span class="lineNum">       5 </span>            : #include &quot;onnx/defs/schema.h&quot;
<span class="lineNum">       6 </span>            : 
<span class="lineNum">       7 </span>            : using namespace ONNX_NAMESPACE;
<span class="lineNum">       8 </span>            : 
<span class="lineNum">       9 </span>            : namespace ONNX_NAMESPACE {
<span class="lineNum">      10 </span>            : 
<span class="lineNum">      11 </span>            : const char* kBroadcastDoc_old = R&quot;DOC(
<span class="lineNum">      12 </span>            : If necessary the right-hand-side argument will be broadcasted to match the
<span class="lineNum">      13 </span>            : shape of left-hand-side argument. When broadcasting is specified, the second
<span class="lineNum">      14 </span>            : tensor can either be of size 1 (a scalar value), or having its shape as a
<span class="lineNum">      15 </span>            : contiguous subset of the first tensor's shape. The starting of the mutually
<span class="lineNum">      16 </span>            : equal shape is specified by the argument &quot;axis&quot;, and if it is not set, suffix
<span class="lineNum">      17 </span>            : matching is assumed. 1-dim expansion doesn't work yet.
<span class="lineNum">      18 </span>            : 
<span class="lineNum">      19 </span>            : For example, the following tensor shapes are supported (with broadcast=1):
<span class="lineNum">      20 </span>            : 
<span class="lineNum">      21 </span>            :   shape(A) = (2, 3, 4, 5), shape(B) = (,), i.e. B is a scalar
<span class="lineNum">      22 </span>            :   shape(A) = (2, 3, 4, 5), shape(B) = (5,)
<span class="lineNum">      23 </span>            :   shape(A) = (2, 3, 4, 5), shape(B) = (4, 5)
<span class="lineNum">      24 </span>            :   shape(A) = (2, 3, 4, 5), shape(B) = (3, 4), with axis=1
<span class="lineNum">      25 </span>            :   shape(A) = (2, 3, 4, 5), shape(B) = (2), with axis=0
<span class="lineNum">      26 </span>            : 
<span class="lineNum">      27 </span>            : Attribute `broadcast=1` needs to be passed to enable broadcasting.
<a name="28"><span class="lineNum">      28 </span>            : )DOC&quot;;</a>
<a name="29"><span class="lineNum">      29 </span>            : </a>
<span class="lineNum">      30 </span>            : std::function&lt;void(OpSchema&amp;)&gt; MathDocGenerator_old(const char* name) {
<span class="lineNum">      31 </span><span class="lineCov">          8 :   return [=](OpSchema&amp; schema) {</span>
<span class="lineNum">      32 </span><span class="lineCov">          4 :     std::string doc = R&quot;DOC(</span>
<span class="lineNum">      33 </span>            : Performs element-wise binary {name} (with limited broadcast support).
<span class="lineNum">      34 </span>            : {broadcast_doc})DOC&quot;;
<span class="lineNum">      35 </span><span class="lineCov">          8 :     ReplaceAll(doc, &quot;{name}&quot;, name);</span>
<span class="lineNum">      36 </span><span class="lineCov">          4 :     ReplaceAll(doc, &quot;{broadcast_doc}&quot;, kBroadcastDoc_old);</span>
<span class="lineNum">      37 </span><span class="lineCov">         12 :     schema.SetDoc(doc);</span>
<span class="lineNum">      38 </span><span class="lineCov">         12 :     schema.Attr(</span>
<span class="lineNum">      39 </span><span class="lineCov">          4 :         &quot;broadcast&quot;,</span>
<span class="lineNum">      40 </span><span class="lineCov">          4 :         &quot;Pass 1 to enable broadcasting&quot;,</span>
<span class="lineNum">      41 </span>            :         AttributeProto::INT,
<span class="lineNum">      42 </span><span class="lineCov">          4 :         static_cast&lt;int64_t&gt;(0));</span>
<span class="lineNum">      43 </span>            : 
<span class="lineNum">      44 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">      45 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">      46 </span>            :     // definition.
<span class="lineNum">      47 </span><span class="lineCov">         12 :     schema.Attr(</span>
<span class="lineNum">      48 </span><span class="lineCov">          4 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">      49 </span><span class="lineCov">          4 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">      50 </span>            :         AttributeProto::INTS,
<span class="lineNum">      51 </span>            :         OPTIONAL);
<span class="lineNum">      52 </span><span class="lineCov">         12 :     schema.Attr(</span>
<span class="lineNum">      53 </span><span class="lineCov">          4 :         &quot;axis&quot;,</span>
<span class="lineNum">      54 </span><span class="lineCov">          4 :         &quot;If set, defines the broadcast dimensions. See doc for details.&quot;,</span>
<span class="lineNum">      55 </span>            :         AttributeProto::INT,
<span class="lineNum">      56 </span>            :         OPTIONAL);
<span class="lineNum">      57 </span><span class="lineCov">         12 :     schema.Input(</span>
<span class="lineNum">      58 </span>            :         0,
<span class="lineNum">      59 </span><span class="lineCov">          4 :         &quot;A&quot;,</span>
<span class="lineNum">      60 </span><span class="lineCov">          4 :         &quot;First operand, should share the type with the second operand.&quot;,</span>
<span class="lineNum">      61 </span><span class="lineCov">          4 :         &quot;T&quot;);</span>
<span class="lineNum">      62 </span><span class="lineCov">         12 :     schema.Input(</span>
<span class="lineNum">      63 </span>            :         1,
<span class="lineNum">      64 </span><span class="lineCov">          4 :         &quot;B&quot;,</span>
<span class="lineNum">      65 </span><span class="lineCov">          4 :         &quot;Second operand. With broadcasting can be of smaller size than A. &quot;</span>
<span class="lineNum">      66 </span>            :         &quot;If broadcasting is disabled it should be of the same size.&quot;,
<span class="lineNum">      67 </span><span class="lineCov">          4 :         &quot;T&quot;);</span>
<span class="lineNum">      68 </span><span class="lineCov">         20 :     schema.Output(0, &quot;C&quot;, &quot;Result, has same dimensions and type as A&quot;, &quot;T&quot;);</span>
<span class="lineNum">      69 </span><span class="lineCov">         28 :     schema.TypeConstraint(</span>
<span class="lineNum">      70 </span><span class="lineCov">          4 :         &quot;T&quot;,</span>
<span class="lineNum">      71 </span><span class="lineCov">         16 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<span class="lineNum">      72 </span><span class="lineCov">          4 :         &quot;Constrain input and output types to float tensors.&quot;);</span>
<span class="lineNum">      73 </span><span class="lineCov">          4 :   };</span>
<a name="74"><span class="lineNum">      74 </span>            : }</a>
<span class="lineNum">      75 </span>            : 
<a name="76"><span class="lineNum">      76 </span><span class="lineCov">          7 : ONNX_OPERATOR_SCHEMA(Add).FillUsing(MathDocGenerator_old(&quot;addition&quot;));</span></a>
<span class="lineNum">      77 </span>            : 
<a name="78"><span class="lineNum">      78 </span><span class="lineCov">          7 : ONNX_OPERATOR_SCHEMA(Sub).FillUsing(MathDocGenerator_old(&quot;subtraction&quot;));</span></a>
<span class="lineNum">      79 </span>            : 
<a name="80"><span class="lineNum">      80 </span><span class="lineCov">          7 : ONNX_OPERATOR_SCHEMA(Mul).FillUsing(MathDocGenerator_old(&quot;multiplication&quot;));</span></a>
<span class="lineNum">      81 </span>            : 
<span class="lineNum">      82 </span><span class="lineCov">          7 : ONNX_OPERATOR_SCHEMA(Div).FillUsing(MathDocGenerator_old(&quot;division&quot;));</span>
<a name="83"><span class="lineNum">      83 </span>            : } // namespace ONNX_NAMESPACE</a>
<span class="lineNum">      84 </span>            : 
<span class="lineNum">      85 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(Neg)</span>
<span class="lineNum">      86 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">      87 </span>            : Neg takes one input data (Tensor&lt;T&gt;) and produces one output data
<span class="lineNum">      88 </span>            : (Tensor&lt;T&gt;) where each element flipped sign, y = -x, is applied to
<span class="lineNum">      89 </span>            : the tensor elementwise.
<span class="lineNum">      90 </span>            : )DOC&quot;)
<span class="lineNum">      91 </span><span class="lineCov">          4 :     .Input(0, &quot;X&quot;, &quot;Input tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">      92 </span><span class="lineCov">          4 :     .Output(0, &quot;Y&quot;, &quot;Output tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">      93 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">      94 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">      95 </span>            :     // definition.
<span class="lineNum">      96 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">      97 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">      98 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">      99 </span>            :         AttributeProto::INTS,
<span class="lineNum">     100 </span>            :         OPTIONAL)
<span class="lineNum">     101 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     102 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     103 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="104"><span class="lineNum">     104 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     105 </span>            : 
<span class="lineNum">     106 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(Abs)</span>
<span class="lineNum">     107 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     108 </span>            : Absolute takes one input data (Tensor&lt;T&gt;) and produces one output data
<span class="lineNum">     109 </span>            : (Tensor&lt;T&gt;) where the absolute is, y = abs(x), is applied to
<span class="lineNum">     110 </span>            : the tensor elementwise.
<span class="lineNum">     111 </span>            : )DOC&quot;)
<span class="lineNum">     112 </span><span class="lineCov">          4 :     .Input(0, &quot;X&quot;, &quot;Input tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     113 </span><span class="lineCov">          4 :     .Output(0, &quot;Y&quot;, &quot;Output tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     114 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     115 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     116 </span>            :     // definition.
<span class="lineNum">     117 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     118 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">     119 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">     120 </span>            :         AttributeProto::INTS,
<span class="lineNum">     121 </span>            :         OPTIONAL)
<span class="lineNum">     122 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     123 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     124 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="125"><span class="lineNum">     125 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     126 </span>            : 
<span class="lineNum">     127 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(Reciprocal)</span>
<span class="lineNum">     128 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     129 </span>            : Reciprocal takes one input data (Tensor&lt;T&gt;) and produces one output data
<span class="lineNum">     130 </span>            : (Tensor&lt;T&gt;) where the reciprocal is, y = 1/x, is applied to
<span class="lineNum">     131 </span>            : the tensor elementwise.
<span class="lineNum">     132 </span>            : )DOC&quot;)
<span class="lineNum">     133 </span><span class="lineCov">          4 :     .Input(0, &quot;X&quot;, &quot;Input tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     134 </span><span class="lineCov">          4 :     .Output(0, &quot;Y&quot;, &quot;Output tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     135 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     136 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     137 </span>            :     // definition.
<span class="lineNum">     138 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     139 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">     140 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">     141 </span>            :         AttributeProto::INTS,
<span class="lineNum">     142 </span>            :         OPTIONAL)
<span class="lineNum">     143 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     144 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     145 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="146"><span class="lineNum">     146 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     147 </span>            : 
<span class="lineNum">     148 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(Floor)</span>
<span class="lineNum">     149 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     150 </span>            : Floor takes one input data (Tensor&lt;T&gt;) and produces one output data
<span class="lineNum">     151 </span>            : (Tensor&lt;T&gt;) where the floor is, y = floor(x), is applied to
<span class="lineNum">     152 </span>            : the tensor elementwise.
<span class="lineNum">     153 </span>            : )DOC&quot;)
<span class="lineNum">     154 </span><span class="lineCov">          4 :     .Input(0, &quot;X&quot;, &quot;Input tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     155 </span><span class="lineCov">          4 :     .Output(0, &quot;Y&quot;, &quot;Output tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     156 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     157 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     158 </span>            :     // definition.
<span class="lineNum">     159 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     160 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">     161 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">     162 </span>            :         AttributeProto::INTS,
<span class="lineNum">     163 </span>            :         OPTIONAL)
<span class="lineNum">     164 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     165 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     166 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="167"><span class="lineNum">     167 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     168 </span>            : 
<span class="lineNum">     169 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(Ceil)</span>
<span class="lineNum">     170 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     171 </span>            : Ceil takes one input data (Tensor&lt;T&gt;) and produces one output data
<span class="lineNum">     172 </span>            : (Tensor&lt;T&gt;) where the ceil is, y = ceil(x), is applied to
<span class="lineNum">     173 </span>            : the tensor elementwise.
<span class="lineNum">     174 </span>            : )DOC&quot;)
<span class="lineNum">     175 </span><span class="lineCov">          4 :     .Input(0, &quot;X&quot;, &quot;Input tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     176 </span><span class="lineCov">          4 :     .Output(0, &quot;Y&quot;, &quot;Output tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     177 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     178 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     179 </span>            :     // definition.
<span class="lineNum">     180 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     181 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">     182 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">     183 </span>            :         AttributeProto::INTS,
<span class="lineNum">     184 </span>            :         OPTIONAL)
<span class="lineNum">     185 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     186 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     187 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="188"><span class="lineNum">     188 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     189 </span>            : 
<span class="lineNum">     190 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(Sqrt)</span>
<span class="lineNum">     191 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     192 </span>            : Square root takes one input data (Tensor&lt;T&gt;) and produces one output data
<span class="lineNum">     193 </span>            : (Tensor&lt;T&gt;) where the square root is, y = x^0.5, is applied to
<span class="lineNum">     194 </span>            : the tensor elementwise. If x is negative, then it will return NaN.
<span class="lineNum">     195 </span>            : )DOC&quot;)
<span class="lineNum">     196 </span><span class="lineCov">          4 :     .Input(0, &quot;X&quot;, &quot;Input tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     197 </span><span class="lineCov">          4 :     .Output(0, &quot;Y&quot;, &quot;Output tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     198 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     199 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     200 </span>            :     // definition.
<span class="lineNum">     201 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     202 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">     203 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">     204 </span>            :         AttributeProto::INTS,
<span class="lineNum">     205 </span>            :         OPTIONAL)
<span class="lineNum">     206 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     207 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     208 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="209"><span class="lineNum">     209 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     210 </span>            : 
<span class="lineNum">     211 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(Relu)</span>
<span class="lineNum">     212 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     213 </span>            : Relu takes one input data (Tensor&lt;T&gt;) and produces one output data
<span class="lineNum">     214 </span>            : (Tensor&lt;T&gt;) where the rectified linear function, y = max(0, x), is applied to
<span class="lineNum">     215 </span>            : the tensor elementwise.
<span class="lineNum">     216 </span>            : )DOC&quot;)
<span class="lineNum">     217 </span><span class="lineCov">          4 :     .Input(0, &quot;X&quot;, &quot;Input tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     218 </span><span class="lineCov">          4 :     .Output(0, &quot;Y&quot;, &quot;Output tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     219 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     220 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     221 </span>            :     // definition.
<span class="lineNum">     222 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     223 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">     224 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">     225 </span>            :         AttributeProto::INTS,
<span class="lineNum">     226 </span>            :         OPTIONAL)
<span class="lineNum">     227 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     228 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     229 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="230"><span class="lineNum">     230 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     231 </span>            : 
<span class="lineNum">     232 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(LeakyRelu)</span>
<span class="lineNum">     233 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     234 </span><span class="lineCov">          1 :         &quot;alpha&quot;,</span>
<span class="lineNum">     235 </span><span class="lineCov">          1 :         &quot;Coefficient of leakage default to 0.01.&quot;,</span>
<span class="lineNum">     236 </span>            :         AttributeProto::FLOAT,
<span class="lineNum">     237 </span><span class="lineCov">          1 :         0.01f)</span>
<span class="lineNum">     238 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     239 </span>            : LeakyRelu takes input data (Tensor&lt;T&gt;) and an argument alpha, and produces one
<span class="lineNum">     240 </span>            : output data (Tensor&lt;T&gt;) where the function `f(x) = alpha * x for x &lt; 0`,
<span class="lineNum">     241 </span>            : `f(x) = x for x &gt;= 0`, is applied to the data tensor elementwise.
<span class="lineNum">     242 </span>            : )DOC&quot;)
<span class="lineNum">     243 </span><span class="lineCov">          4 :     .Input(0, &quot;X&quot;, &quot;Input tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     244 </span><span class="lineCov">          4 :     .Output(0, &quot;Y&quot;, &quot;Output tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     245 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     246 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     247 </span>            :     // definition.
<span class="lineNum">     248 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     249 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">     250 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">     251 </span>            :         AttributeProto::INTS,
<span class="lineNum">     252 </span>            :         OPTIONAL)
<span class="lineNum">     253 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     254 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     255 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="256"><span class="lineNum">     256 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     257 </span>            : 
<span class="lineNum">     258 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(Selu)</span>
<span class="lineNum">     259 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     260 </span><span class="lineCov">          1 :         &quot;alpha&quot;,</span>
<span class="lineNum">     261 </span><span class="lineCov">          1 :         &quot;Coefficient of SELU default to 1.6732.&quot;,</span>
<span class="lineNum">     262 </span>            :         AttributeProto::FLOAT,
<span class="lineNum">     263 </span><span class="lineCov">          1 :         1.6732f)</span>
<span class="lineNum">     264 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     265 </span><span class="lineCov">          1 :         &quot;gamma&quot;,</span>
<span class="lineNum">     266 </span><span class="lineCov">          1 :         &quot;Coefficient of SELU default to 1.0507.&quot;,</span>
<span class="lineNum">     267 </span>            :         AttributeProto::FLOAT,
<span class="lineNum">     268 </span><span class="lineCov">          1 :         1.0507f)</span>
<span class="lineNum">     269 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     270 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     271 </span>            :     // definition.
<span class="lineNum">     272 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     273 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">     274 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">     275 </span>            :         AttributeProto::INTS,
<span class="lineNum">     276 </span>            :         OPTIONAL)
<span class="lineNum">     277 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     278 </span>            : Selu takes one input data (Tensor&lt;T&gt;) and produces one output data
<span class="lineNum">     279 </span>            : (Tensor&lt;T&gt;) where the scaled exponential linear unit function,
<span class="lineNum">     280 </span>            : `y = gamma * (alpha * e^x - alpha) for x &lt;= 0`, `y = gamma * x for x &gt; 0`,
<span class="lineNum">     281 </span>            : is applied to the tensor elementwise.
<span class="lineNum">     282 </span>            : )DOC&quot;)
<span class="lineNum">     283 </span><span class="lineCov">          4 :     .Input(0, &quot;X&quot;, &quot;Input tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     284 </span><span class="lineCov">          4 :     .Output(0, &quot;Y&quot;, &quot;Output tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     285 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     286 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     287 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="288"><span class="lineNum">     288 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     289 </span>            : 
<span class="lineNum">     290 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(Elu)</span>
<span class="lineNum">     291 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     292 </span><span class="lineCov">          1 :         &quot;alpha&quot;,</span>
<span class="lineNum">     293 </span><span class="lineCov">          1 :         &quot;Coefficient of ELU default to 1.0.&quot;,</span>
<span class="lineNum">     294 </span>            :         AttributeProto::FLOAT,
<span class="lineNum">     295 </span><span class="lineCov">          1 :         1.0f)</span>
<span class="lineNum">     296 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     297 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     298 </span>            :     // definition.
<span class="lineNum">     299 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     300 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">     301 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">     302 </span>            :         AttributeProto::INTS,
<span class="lineNum">     303 </span>            :         OPTIONAL)
<span class="lineNum">     304 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     305 </span>            : Elu takes one input data (Tensor&lt;T&gt;) and produces one output data
<span class="lineNum">     306 </span>            : (Tensor&lt;T&gt;) where the function `f(x) = alpha * (exp(x) - 1.) for x &lt;
<span class="lineNum">     307 </span>            : 0`, `f(x) = x for x &gt;= 0`., is applied to the tensor elementwise.
<span class="lineNum">     308 </span>            : 
<span class="lineNum">     309 </span>            : )DOC&quot;)
<span class="lineNum">     310 </span><span class="lineCov">          4 :     .Input(0, &quot;X&quot;, &quot;1D input tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     311 </span><span class="lineCov">          4 :     .Output(0, &quot;Y&quot;, &quot;1D input tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     312 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     313 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     314 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="315"><span class="lineNum">     315 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     316 </span>            : 
<span class="lineNum">     317 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(Exp)</span>
<span class="lineNum">     318 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     319 </span>            : Calculates the exponential of the given input tensor, element-wise.
<span class="lineNum">     320 </span>            : )DOC&quot;)
<span class="lineNum">     321 </span><span class="lineCov">          4 :     .Input(0, &quot;input&quot;, &quot;Input tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     322 </span><span class="lineCov">          1 :     .Output(</span>
<span class="lineNum">     323 </span>            :         0,
<span class="lineNum">     324 </span><span class="lineCov">          1 :         &quot;output&quot;,</span>
<span class="lineNum">     325 </span><span class="lineCov">          1 :         &quot;The exponential of the input tensor computed &quot;</span>
<span class="lineNum">     326 </span>            :         &quot;element-wise&quot;,
<span class="lineNum">     327 </span><span class="lineCov">          1 :         &quot;T&quot;)</span>
<span class="lineNum">     328 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     329 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     330 </span>            :     // definition.
<span class="lineNum">     331 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     332 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">     333 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">     334 </span>            :         AttributeProto::INTS,
<span class="lineNum">     335 </span>            :         OPTIONAL)
<span class="lineNum">     336 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     337 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     338 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="339"><span class="lineNum">     339 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     340 </span>            : 
<span class="lineNum">     341 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(Log)</span>
<span class="lineNum">     342 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     343 </span>            : Calculates the natural log of the given input tensor, element-wise.
<span class="lineNum">     344 </span>            : )DOC&quot;)
<span class="lineNum">     345 </span><span class="lineCov">          4 :     .Input(0, &quot;input&quot;, &quot;Input tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     346 </span><span class="lineCov">          1 :     .Output(</span>
<span class="lineNum">     347 </span>            :         0,
<span class="lineNum">     348 </span><span class="lineCov">          1 :         &quot;output&quot;,</span>
<span class="lineNum">     349 </span><span class="lineCov">          1 :         &quot;The natural log of the input tensor computed &quot;</span>
<span class="lineNum">     350 </span>            :         &quot;element-wise&quot;,
<span class="lineNum">     351 </span><span class="lineCov">          1 :         &quot;T&quot;)</span>
<span class="lineNum">     352 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     353 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     354 </span>            :     // definition.
<span class="lineNum">     355 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     356 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">     357 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">     358 </span>            :         AttributeProto::INTS,
<span class="lineNum">     359 </span>            :         OPTIONAL)
<span class="lineNum">     360 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     361 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     362 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="363"><span class="lineNum">     363 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     364 </span>            : 
<span class="lineNum">     365 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(Tanh)</span>
<span class="lineNum">     366 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     367 </span>            : Calculates the hyperbolic tangent of the given input tensor element-wise.
<span class="lineNum">     368 </span>            : )DOC&quot;)
<span class="lineNum">     369 </span><span class="lineCov">          4 :     .Input(0, &quot;input&quot;, &quot;1-D input tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     370 </span><span class="lineCov">          1 :     .Output(</span>
<span class="lineNum">     371 </span>            :         0,
<span class="lineNum">     372 </span><span class="lineCov">          1 :         &quot;output&quot;,</span>
<span class="lineNum">     373 </span><span class="lineCov">          1 :         &quot;The hyperbolic tangent values of the input tensor &quot;</span>
<span class="lineNum">     374 </span>            :         &quot;computed element-wise&quot;,
<span class="lineNum">     375 </span><span class="lineCov">          1 :         &quot;T&quot;)</span>
<span class="lineNum">     376 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     377 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     378 </span>            :     // definition.
<span class="lineNum">     379 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     380 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">     381 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">     382 </span>            :         AttributeProto::INTS,
<span class="lineNum">     383 </span>            :         OPTIONAL)
<span class="lineNum">     384 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     385 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     386 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="387"><span class="lineNum">     387 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     388 </span>            : 
<span class="lineNum">     389 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(PRelu)</span>
<span class="lineNum">     390 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     391 </span>            : 
<span class="lineNum">     392 </span>            : PRelu takes input data (Tensor&lt;T&gt;) and slope tensor as input, and produces one
<span class="lineNum">     393 </span>            : output data (Tensor&lt;T&gt;) where the function `f(x) = slope * x for x &lt; 0`,
<span class="lineNum">     394 </span>            : `f(x) = x for x &gt;= 0`., is applied to the data tensor elementwise.
<span class="lineNum">     395 </span>            : 
<span class="lineNum">     396 </span>            : )DOC&quot;)
<span class="lineNum">     397 </span><span class="lineCov">          4 :     .Input(0, &quot;X&quot;, &quot;Input tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     398 </span><span class="lineCov">          1 :     .Input(</span>
<span class="lineNum">     399 </span>            :         1,
<span class="lineNum">     400 </span><span class="lineCov">          1 :         &quot;slope&quot;,</span>
<span class="lineNum">     401 </span><span class="lineCov">          1 :         &quot;Slope tensor. If `Slope` is of size 1, the value is shared&quot;</span>
<span class="lineNum">     402 </span>            :         &quot;across different channels&quot;,
<span class="lineNum">     403 </span><span class="lineCov">          1 :         &quot;T&quot;)</span>
<span class="lineNum">     404 </span><span class="lineCov">          4 :     .Output(0, &quot;Y&quot;, &quot;Output tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     405 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     406 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     407 </span>            :     // definition.
<span class="lineNum">     408 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     409 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">     410 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">     411 </span>            :         AttributeProto::INTS,
<span class="lineNum">     412 </span>            :         OPTIONAL)
<span class="lineNum">     413 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     414 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     415 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="416"><span class="lineNum">     416 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     417 </span>            : 
<span class="lineNum">     418 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(Sigmoid)</span>
<span class="lineNum">     419 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     420 </span>            : Sigmoid takes one input data (Tensor&lt;T&gt;) and produces one output data
<span class="lineNum">     421 </span>            : (Tensor&lt;T&gt;) where the sigmoid function, y = 1 / (1 + exp(-x)), is applied to the
<span class="lineNum">     422 </span>            : tensor elementwise.
<span class="lineNum">     423 </span>            : )DOC&quot;)
<span class="lineNum">     424 </span><span class="lineCov">          4 :     .Input(0, &quot;X&quot;, &quot;Input tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     425 </span><span class="lineCov">          4 :     .Output(0, &quot;Y&quot;, &quot;Output tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     426 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     427 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     428 </span>            :     // definition.
<span class="lineNum">     429 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     430 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">     431 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">     432 </span>            :         AttributeProto::INTS,
<span class="lineNum">     433 </span>            :         OPTIONAL)
<span class="lineNum">     434 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     435 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     436 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="437"><span class="lineNum">     437 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     438 </span>            : 
<span class="lineNum">     439 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(HardSigmoid)</span>
<span class="lineNum">     440 </span><span class="lineCov">          3 :     .Attr(&quot;alpha&quot;, &quot;Value of alpha default to 0.2&quot;, AttributeProto::FLOAT, 0.2f)</span>
<span class="lineNum">     441 </span><span class="lineCov">          3 :     .Attr(&quot;beta&quot;, &quot;Value of beta default to 0.5&quot;, AttributeProto::FLOAT, 0.5f)</span>
<span class="lineNum">     442 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     443 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     444 </span>            :     // definition.
<span class="lineNum">     445 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     446 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">     447 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">     448 </span>            :         AttributeProto::INTS,
<span class="lineNum">     449 </span>            :         OPTIONAL)
<span class="lineNum">     450 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     451 </span>            : HardSigmoid takes one input data (Tensor&lt;T&gt;) and produces one output data
<span class="lineNum">     452 </span>            : (Tensor&lt;T&gt;) where the HardSigmoid function, y = max(0, min(1, alpha * x + beta)),
<span class="lineNum">     453 </span>            : is applied to the tensor elementwise.
<span class="lineNum">     454 </span>            : )DOC&quot;)
<span class="lineNum">     455 </span><span class="lineCov">          4 :     .Input(0, &quot;X&quot;, &quot;Input tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     456 </span><span class="lineCov">          4 :     .Output(0, &quot;Y&quot;, &quot;Output tensor&quot;, &quot;T&quot;)</span>
<span class="lineNum">     457 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     458 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     459 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="460"><span class="lineNum">     460 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     461 </span>            : 
<span class="lineNum">     462 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(Max)</span>
<span class="lineNum">     463 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     464 </span>            : Element-wise max of each of the input tensors. All inputs and outputs must
<span class="lineNum">     465 </span>            : have the same shape and data type.
<span class="lineNum">     466 </span>            : )DOC&quot;)
<span class="lineNum">     467 </span><span class="lineCov">          4 :     .Input(0, &quot;data_0&quot;, &quot;List of tensors for Max.&quot;, &quot;T&quot;, OpSchema::Variadic)</span>
<span class="lineNum">     468 </span><span class="lineCov">          4 :     .Output(0, &quot;max&quot;, &quot;Output tensor. Same dimension as inputs.&quot;, &quot;T&quot;)</span>
<span class="lineNum">     469 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     470 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     471 </span>            :     // definition.
<span class="lineNum">     472 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     473 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">     474 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">     475 </span>            :         AttributeProto::INTS,
<span class="lineNum">     476 </span>            :         OPTIONAL)
<span class="lineNum">     477 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     478 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     479 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="480"><span class="lineNum">     480 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     481 </span>            : 
<span class="lineNum">     482 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(Min)</span>
<span class="lineNum">     483 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     484 </span>            : Element-wise min of each of the input tensors. All inputs and outputs must
<span class="lineNum">     485 </span>            : have the same shape and data type.
<span class="lineNum">     486 </span>            : )DOC&quot;)
<span class="lineNum">     487 </span><span class="lineCov">          4 :     .Input(0, &quot;data_0&quot;, &quot;List of tensors for Min&quot;, &quot;T&quot;, OpSchema::Variadic)</span>
<span class="lineNum">     488 </span><span class="lineCov">          4 :     .Output(0, &quot;min&quot;, &quot;Output tensor. Same dimension as inputs.&quot;, &quot;T&quot;)</span>
<span class="lineNum">     489 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     490 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     491 </span>            :     // definition.
<span class="lineNum">     492 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     493 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">     494 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">     495 </span>            :         AttributeProto::INTS,
<span class="lineNum">     496 </span>            :         OPTIONAL)
<span class="lineNum">     497 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     498 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     499 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="500"><span class="lineNum">     500 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     501 </span>            : 
<span class="lineNum">     502 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(Sum)</span>
<span class="lineNum">     503 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     504 </span>            : Element-wise sum of each of the input tensors. All inputs and outputs must
<span class="lineNum">     505 </span>            : have the same shape and data type.
<span class="lineNum">     506 </span>            : )DOC&quot;)
<span class="lineNum">     507 </span><span class="lineCov">          4 :     .Input(0, &quot;data_0&quot;, &quot;List of tensors for Sum.&quot;, &quot;T&quot;, OpSchema::Variadic)</span>
<span class="lineNum">     508 </span><span class="lineCov">          4 :     .Output(0, &quot;sum&quot;, &quot;Output tensor. Same dimension as inputs.&quot;, &quot;T&quot;)</span>
<span class="lineNum">     509 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     510 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     511 </span>            :     // definition.
<span class="lineNum">     512 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     513 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">     514 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">     515 </span>            :         AttributeProto::INTS,
<span class="lineNum">     516 </span>            :         OPTIONAL)
<span class="lineNum">     517 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     518 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     519 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="520"><span class="lineNum">     520 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     521 </span>            : 
<span class="lineNum">     522 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(Mean)</span>
<span class="lineNum">     523 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     524 </span>            : Element-wise mean of each of the input tensors. All inputs and outputs must
<span class="lineNum">     525 </span>            : have the same shape and data type.
<span class="lineNum">     526 </span>            : )DOC&quot;)
<span class="lineNum">     527 </span><span class="lineCov">          4 :     .Input(0, &quot;data_0&quot;, &quot;List of tensors for Mean.&quot;, &quot;T&quot;, OpSchema::Variadic)</span>
<span class="lineNum">     528 </span><span class="lineCov">          4 :     .Output(0, &quot;mean&quot;, &quot;Output tensor. Same dimension as inputs.&quot;, &quot;T&quot;)</span>
<span class="lineNum">     529 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     530 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     531 </span>            :     // definition.
<span class="lineNum">     532 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     533 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">     534 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">     535 </span>            :         AttributeProto::INTS,
<span class="lineNum">     536 </span>            :         OPTIONAL)
<span class="lineNum">     537 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     538 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     539 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="540"><span class="lineNum">     540 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     541 </span>            : 
<span class="lineNum">     542 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(Clip)</span>
<span class="lineNum">     543 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(</span>
<span class="lineNum">     544 </span>            : Clip operator limits the given input within an interval. The interval is
<span class="lineNum">     545 </span>            : specified with arguments 'min' and 'max'. They default to
<span class="lineNum">     546 </span>            : numeric_limits::lowest() and numeric_limits::max() respectively.
<span class="lineNum">     547 </span>            : )DOC&quot;)
<span class="lineNum">     548 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     549 </span><span class="lineCov">          1 :         &quot;min&quot;,</span>
<span class="lineNum">     550 </span><span class="lineCov">          1 :         &quot;Minimum value, under which element is replaced by min&quot;,</span>
<span class="lineNum">     551 </span>            :         AttributeProto::FLOAT,
<span class="lineNum">     552 </span>            :         OPTIONAL)
<span class="lineNum">     553 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     554 </span><span class="lineCov">          1 :         &quot;max&quot;,</span>
<span class="lineNum">     555 </span><span class="lineCov">          1 :         &quot;Maximum value, above which element is replaced by max&quot;,</span>
<span class="lineNum">     556 </span>            :         AttributeProto::FLOAT,
<span class="lineNum">     557 </span>            :         OPTIONAL)
<span class="lineNum">     558 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     559 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     560 </span>            :     // definition.
<span class="lineNum">     561 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     562 </span><span class="lineCov">          1 :         &quot;consumed_inputs&quot;,</span>
<span class="lineNum">     563 </span><span class="lineCov">          1 :         &quot;legacy optimization attribute.&quot;,</span>
<span class="lineNum">     564 </span>            :         AttributeProto::INTS,
<span class="lineNum">     565 </span>            :         OPTIONAL)
<span class="lineNum">     566 </span><span class="lineCov">          4 :     .Input(0, &quot;input&quot;, &quot;Input tensor whose elements to be clipped&quot;, &quot;T&quot;)</span>
<span class="lineNum">     567 </span><span class="lineCov">          4 :     .Output(0, &quot;output&quot;, &quot;Output tensor with clipped input elements&quot;, &quot;T&quot;)</span>
<span class="lineNum">     568 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     569 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     570 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<a name="571"><span class="lineNum">     571 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;);</span></a>
<span class="lineNum">     572 </span>            : 
<span class="lineNum">     573 </span><span class="lineCov">          9 : ONNX_OPERATOR_SCHEMA(Gemm)</span>
<span class="lineNum">     574 </span><span class="lineCov">          2 :     .SetDoc(R&quot;DOC(General Matrix multiplication:</span>
<span class="lineNum">     575 </span>            : https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_3
<span class="lineNum">     576 </span>            : Compute Y = alpha * A * B + beta * C, where input tensor A has dimension (M X K)
<span class="lineNum">     577 </span>            : , input tensor B has dimension (K X N), input tensor C and output tensor Y have
<span class="lineNum">     578 </span>            : dimension (M X N).
<span class="lineNum">     579 </span>            : If attribute broadcast is non-zero, input tensor C will be broadcasted to match
<span class="lineNum">     580 </span>            : the dimension requirement. A will be transposed before doing the computation
<span class="lineNum">     581 </span>            : if attribute transA is non-zero, same for B and transB.
<span class="lineNum">     582 </span>            : )DOC&quot;)
<span class="lineNum">     583 </span><span class="lineCov">          4 :     .Input(0, &quot;A&quot;, &quot;Input tensor A&quot;, &quot;T&quot;)</span>
<span class="lineNum">     584 </span><span class="lineCov">          4 :     .Input(1, &quot;B&quot;, &quot;Input tensor B&quot;, &quot;T&quot;)</span>
<span class="lineNum">     585 </span><span class="lineCov">          4 :     .Input(2, &quot;C&quot;, &quot;Input tensor C, can be inplace.&quot;, &quot;T&quot;)</span>
<span class="lineNum">     586 </span><span class="lineCov">          4 :     .Output(0, &quot;Y&quot;, &quot;Output tensor.&quot;, &quot;T&quot;)</span>
<span class="lineNum">     587 </span><span class="lineCov">          1 :     .TypeConstraint(</span>
<span class="lineNum">     588 </span><span class="lineCov">          1 :         &quot;T&quot;,</span>
<span class="lineNum">     589 </span><span class="lineCov">          4 :         {&quot;tensor(float16)&quot;, &quot;tensor(float)&quot;, &quot;tensor(double)&quot;},</span>
<span class="lineNum">     590 </span><span class="lineCov">          1 :         &quot;Constrain input and output types to float tensors.&quot;)</span>
<span class="lineNum">     591 </span>            :     // This attribute was added via AllowConsumed API in OpSchema.
<span class="lineNum">     592 </span>            :     // After removing the API, we're now using the Attr API to simulate the old
<span class="lineNum">     593 </span>            :     // definition.
<span class="lineNum">     594 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     595 </span><span class="lineCov">          1 :         &quot;transA&quot;,</span>
<span class="lineNum">     596 </span><span class="lineCov">          1 :         &quot;Whether A should be transposed&quot;,</span>
<span class="lineNum">     597 </span>            :         AttributeProto::INT,
<span class="lineNum">     598 </span><span class="lineCov">          1 :         static_cast&lt;int64_t&gt;(0))</span>
<span class="lineNum">     599 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     600 </span><span class="lineCov">          1 :         &quot;transB&quot;,</span>
<span class="lineNum">     601 </span><span class="lineCov">          1 :         &quot;Whether B should be transposed&quot;,</span>
<span class="lineNum">     602 </span>            :         AttributeProto::INT,
<span class="lineNum">     603 </span><span class="lineCov">          1 :         static_cast&lt;int64_t&gt;(0))</span>
<span class="lineNum">     604 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     605 </span><span class="lineCov">          1 :         &quot;broadcast&quot;,</span>
<span class="lineNum">     606 </span><span class="lineCov">          1 :         &quot;Whether C should be broadcasted&quot;,</span>
<span class="lineNum">     607 </span>            :         AttributeProto::INT,
<span class="lineNum">     608 </span><span class="lineCov">          1 :         static_cast&lt;int64_t&gt;(0))</span>
<span class="lineNum">     609 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     610 </span><span class="lineCov">          1 :         &quot;alpha&quot;,</span>
<span class="lineNum">     611 </span><span class="lineCov">          1 :         &quot;Scalar multiplier for the product of input tensors A * B&quot;,</span>
<span class="lineNum">     612 </span>            :         AttributeProto::FLOAT,
<span class="lineNum">     613 </span><span class="lineCov">          1 :         1.0f)</span>
<span class="lineNum">     614 </span><span class="lineCov">          1 :     .Attr(</span>
<span class="lineNum">     615 </span><span class="lineCov">          1 :         &quot;beta&quot;,</span>
<span class="lineNum">     616 </span><span class="lineCov">          1 :         &quot;Scalar multiplier for input tensor C&quot;,</span>
<span class="lineNum">     617 </span>            :         AttributeProto::FLOAT,
<span class="lineNum">     618 </span><span class="lineCov">          1 :         1.0f);</span>
</pre>
      </td>
    </tr>
  </table>
  <br>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="ruler"><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>
    <tr><td class="versionInfo">Generated by: <a href="http://ltp.sourceforge.net/coverage/lcov.php" target="_parent">LCOV version 1.13</a></td></tr>
  </table>
  <br>

</body>
</html>
